# -*- coding: utf-8 -*-
"""모델링_500_주현님.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1v1NC9dsGMxghaXa2EjVGomYsgQ4lO3sw
"""

!sudo apt-get install -y fonts-NanumBarunGothic
!sudo fc-cache -fv
!rm ~/.cache/matplotlib -rf

import pandas as pd
import numpy as np

#머신러닝에 필요한 기본 패키지 설치
!pip install geopandas
!pip install scipy
!pip install matplotlib
!pip install ipython
!pip install scikit-learn
!pip install pillow
!pip install wrapt
!pip install tensorflow==2.0.0
!pip install mglearn

# Commented out IPython magic to ensure Python compatibility.
#머신러닝 데이터 적재
from os import listdir
from os.path import isfile, join
import pandas as pd
import geopandas as gpd
import time
import json
import folium
import matplotlib.pyplot as plt
# %matplotlib inline

import numpy as np
import seaborn as sns
import fiona
from shapely.geometry.multipolygon import MultiPolygon
from shapely.geometry import Point, Polygon, LineString
from shapely import wkt

from tqdm import trange, tqdm, tqdm_notebook, tnrange

from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression

from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score
from sklearn.preprocessing import Binarizer, MinMaxScaler
import sklearn.metrics as metrics
from lightgbm import LGBMClassifier
import xgboost as xgb

from imblearn.over_sampling import SMOTE
from functools import partial

import matplotlib as mpl
import matplotlib.pyplot as plt
import seaborn as sns

import warnings

"""# **[Train Dataset]**

# 학습용 데이터 불러오기


*   250격자 고양시(0값 제거)
*   500격자 고양시(0값 제거)
"""

df = pd.read_csv('/content/500격자 고양시(0값 제거).csv',  encoding='UTF-8')
df.head(10)

len(df)

df= df.fillna(0)

"""결측치/결측값(missing value) 시각화"""

import missingno as msno
msno.matrix(df)

"""#상관관계 확인


*   각 변수들끼리의 상관관계를 확인하고 시각화 해서 종속변수와 상관관계가 높은 변수들만 선택



"""

import matplotlib as mpl
import matplotlib.pyplot as plt
import seaborn as sns
import warnings

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
# %config InlineBackend.figure_format = 'retina'

mpl.rc('font', family='NanumGothic') # 폰트 설정
mpl.rc('axes', unicode_minus=False) # 유니코드에서 음수 부호 설정

# 차트 스타일 설정
sns.set(font="NanumGothic", rc={"axes.unicode_minus":False}, style='darkgrid')
plt.rc("figure", figsize=(10,8))

warnings.filterwarnings("ignore")

X = df.iloc[:, 3:-2]
y = df.iloc[:, -1]

# matplotlib 한글 지원 폰트 설정
plt.rc('font',family = 'NanumBarunGothic')

import seaborn as sns
plt.figure(figsize=(12,10))

sns.heatmap(df.corr(), annot=True, vmin=-1, vmax=1, cmap="coolwarm_r")

plt.show()

"""#Target = 정류장유무 지정


"""

target = df['정류장 유무']
df

"""Target 값 시각화"""

plt.bar(target.unique(), target.value_counts())
plt.xticks([0, 1])
plt.show()
print(target.value_counts())

df

"""필요없는 컬럼값 제거"""

df = df.drop(['geometry'], axis=1)
df = df.drop([df.columns[0]], axis=1)
df = df.drop(['정류장 유무'], axis =1)
df = df.drop(['정류장 개수'], axis =1)


df.head(5)

"""# 분류모형의 Feature Importance

정규화(Feature Scaling)
"""

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
df[:] = scaler.fit_transform(df[:])

df

from sklearn.model_selection import train_test_split

data_train = df
target_train = target

#랜덤포레스트
from sklearn.ensemble import RandomForestClassifier
rf_model = RandomForestClassifier(n_estimators=20, random_state = 22)
rf_model.fit(data_train, target_train)


features = pd.DataFrame(data=np.c_[X.columns.values,rf_model.feature_importances_],
                        columns=["feature", "importance"])



features.sort_values(by="importance", ascending=False, inplace=True)
features.reset_index(drop=True, inplace=True)
features

from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix
from sklearn.metrics import f1_score, roc_auc_score

"""# **[Test Dataset]**

# 테스트용 데이터 불러오기


*   250격자 세종시(0값 제거)
*   500격자 세종시(0값 제거)
"""

#데이터 불러오기
s_df = pd.read_csv('/content/500격자 세종시.csv',  encoding='UTF-8')
s_df.head(10)
len(s_df)

import missingno as msno

msno.matrix(s_df)

"""#Target = 정류장 유무 지정


"""

S_target = s_df['정류장 유무']
s_df

"""Test target 결과값 시각화"""

plt.bar(S_target.unique(), S_target.value_counts())
plt.xticks([0, 1])
plt.show()
print(S_target.value_counts())

s_df.columns

"""필요없는 column값 제거"""

s_df = s_df.drop(['geometry'], axis=1)
s_df = s_df.drop(['Unnamed: 0'], axis=1)
s_df = s_df.drop(['정류장 유무'], axis =1)
s_df = s_df.drop(['정류장 개수'], axis =1)
s_df = s_df.drop(['출발지도착지'], axis =1)


s_df.head(5)

"""정규화(Feature Scaling)"""

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
s_df[:] = scaler.fit_transform(s_df[:])

s_df

data_test = s_df
target_test = S_target

"""SMOTE 적용"""

from imblearn.over_sampling import SMOTE
smote = SMOTE(random_state=22)
data_train,target_train = smote.fit_resample(data_train,target_train)
print('SMOTE 적용 전 학습용 피처/레이블 데이터 세트: ', data_train.shape, target_train.shape)
print('SMOTE 적용 후 학습용 피처/레이블 데이터 세트: ', data_train_over.shape, target_train_over.shape)
print('SMOTE 적용 후 레이블 값 분포: \n', pd.Series(target_train_over).value_counts())

#나오는 방식 만들기
from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix
from sklearn.metrics import f1_score, roc_auc_score

def get_clf_eval(y_test, pred=None, pred_proba_po=None):
    confusion = confusion_matrix(y_test, pred)
    accuracy = accuracy_score(y_test, pred)
    precision = precision_score(y_test, pred)
    recall = recall_score(y_test, pred)
    f1 = f1_score(y_test, pred)
    auc = roc_auc_score(y_test, pred_proba_po)

    print("오차 행렬")
    print(confusion)
    print(f"정확도: {accuracy:.4f}, 정밀도: {precision:.4f}, 재현율: {recall:.4f}, F1: {f1:.4f}, AUC: {auc:.4f}")

"""#DecisionTree Classifier"""

numNeighbors = list(range(1, 30))
trainF1 = []  #아무것도 들어있지 않는 빈 리스트
testF1 = []
for k in numNeighbors:
    clf = DecisionTreeClassifier(max_depth=k, random_state=22)
    clf.fit(data_train, target_train)
    Y_predTrain = clf.predict(data_train)
    Y_predTest = clf.predict(data_test)
    trainF1.append(f1_score(target_train, Y_predTrain,average='micro'))
    testF1.append(f1_score(target_test, Y_predTest))
plt.figure(figsize=(15, 6))
plt.plot(numNeighbors, trainF1, 'ro-', numNeighbors, testF1, 'bv--')
plt.legend(['Train F1', 'Test F1'])
plt.xlabel('Number of Max_depth')
plt.ylabel('F1 score')
plt.show()
print("best testF1 :", max(testF1))

#test
numNeighbors = list(range(1, 30))

thresholds = np.arange(0,1,0.1)

for threshold in thresholds:
  trainF1 = []  #아무것도 들어있지 않는 빈 리스트
  testF1 = []





  for k in numNeighbors:
      clf = DecisionTreeClassifier(max_depth=k, random_state=22)
      clf.fit(data_train_over, target_train_over)
      Y_predproba_train = clf.predict_proba(data_train_over)
      Y_predproba_test = clf.predict_proba(data_test)
      Y_predTrain = (Y_predproba_train [:,1] >= threshold).astype('int')
      Y_predTest = (Y_predproba_test [:,1] >= threshold).astype('int')
      trainF1.append(f1_score(target_train_over, Y_predTrain, average='micro'))
      testF1.append(f1_score(target_test, Y_predTest))
  plt.figure(figsize=(15, 6))
  plt.plot(numNeighbors, trainF1, 'ro-', numNeighbors, testF1, 'bv--')
  plt.legend(['Train F1', 'Test F1'])
  xlabel = 'threshold :', threshold,'Number of Max_depth'
  plt.xlabel(xlabel)
  plt.ylabel('F1 score')
  plt.show()
  print("best testF1 :", max(testF1))

s_df = data_test.copy()
s_df['정류장 유무']=target_test
s_df

s_df['0일확률'] = clf.predict_proba(data_test)[:,0]
s_df['1확률'] = clf.predict_proba(data_test)[:,1]

s_df

max(testF1)

testF1.index(max(testF1))

testF1.index(max(testF1))+1

s_df_nonull = pd.read_csv('/content/500격자 세종시.csv',  encoding='UTF-8')
#df = pd.drop([pd.columns[0]], axis=1)
s_df_nonull.head(10)

import shapely.wkt
import geopandas
s_df_nonull['geometry'] = s_df_nonull['geometry'].astype('string')
P = []

for s in s_df_nonull['geometry']:
    P.append(shapely.wkt.loads(s))

s_df_nonull['geometry'] = P
s_df_nonull = geopandas.GeoDataFrame(s_df_nonull, geometry=P)

# threshold, max_depth 값을 직접 넣음

clf = DecisionTreeClassifier(max_depth=1, random_state=22)
clf.fit(data_train_over, target_train_over)
Y_predproba_test = clf.predict_proba(data_test)
Y_predTest = (Y_predproba_test [:,1] >= 0.5).astype('int')

temp = s_df_nonull.copy()
temp['pred'] = [0]*len(temp)
for i, idx in enumerate(data_test.index):
    temp['pred'][idx] = Y_predTest[i]

print('실제 정류장이 있는 격자 수 :', len(temp[temp['정류장 유무']==1]))
print('정류장이 있어야 한다고 분류한 타일 수 :', len(temp[temp['pred']==1]))
print('recall score 실제 정류장이 맞는 지 맞춘 경우 :', len(temp[(temp['pred']==1) & (temp['정류장 유무']==1)]), '/', len(temp[temp['정류장 유무']==1]))
print('precision score 실제 정류장이 없는 곳을 맞춘 경우 :', len(temp[(temp['pred']==0) & (temp['정류장 유무']==0)]), '/', len(temp[temp['정류장 유무']==0]))
print('최적 입지 후보(실제론 없지만, 있어야 한다고 분류) :', len(temp[(temp['pred']==1) & (temp['정류장 유무']==0)]), '/', len(temp[temp['정류장 유무']==0]))

# 정류장 유무 == 0 & predict == 1 인 경우만 ['pred']에서 1로 설정
idx = temp[(temp['정류장 유무'] == 0) & (temp['pred'] == 1)].index
for i in temp[temp['pred'] == 1].index:
    temp['pred'].loc[i] = 0
for i in idx:
    temp['pred'].loc[i] = 1

fig, ax1 = plt.subplots(figsize=(15, 15))
fig.set_facecolor('white')
s_df_nonull['geometry'].plot(ax=ax1, color='gainsboro')
s_df_nonull[s_df_nonull['정류장 유무']==1].plot(ax=ax1, column='정류장 유무', color='plum', aspect=1)
temp[temp['pred']==1].plot(ax=ax1, column='pred', color='lightskyblue', aspect=1)
ax1.set_axis_off()
plt.show()

clf = DecisionTreeClassifier(max_depth=testF1.index(max(testF1))+1, random_state=22)
clf.fit(data_train, target_train)


print("RF 훈련 세트 정확도: {:.3f}".format(clf.score(data_train, target_train)))
print("RF 테스트 세트 정확도: {:.3f}".format(clf.score(data_test, target_test)))
# 예측
preds = clf.predict(data_test)
pred_proba = clf.predict_proba(data_test)[:,1]

# 평가
get_clf_eval(target_test, preds, pred_proba)

s_df[s_df['정류장 유무']==0].index

#s_df['pred']=preds
#s_df

#s_df = s_df[(s_df['정류장 유무']==0)&(s_df['pred']==1)]

s_df.index

"""#KNN"""

import matplotlib.pyplot as plt

numNeighbors = list(range(1, 30))
trainF1 = []
testF1 = []
for k in numNeighbors:
    clf = KNeighborsClassifier(n_neighbors=k, metric='minkowski')
    clf.fit(data_train, target_train)
    Y_predTrain = clf.predict(data_train)
    Y_predTest = clf.predict(data_test)
    trainF1.append(f1_score(target_train, Y_predTrain, average='micro'))
    testF1.append(f1_score(target_test, Y_predTest, average='micro'))
plt.figure(figsize=(15, 6))
plt.plot(numNeighbors, trainF1, 'ro-', numNeighbors, testF1, 'bv--')
plt.legend(['Train F1', 'Test F1'])
plt.xlabel('Number of neighbors')
plt.ylabel('F1 score')
plt.show()
print("best testF1 :", max(testF1))

print("RF 훈련 세트 정확도: {:.3f}".format(clf.score(data_train, target_train)))
print("RF 테스트 세트 정확도: {:.3f}".format(clf.score(data_test, target_test)))

# 예측
preds = clf.predict(data_test)
pred_proba = clf.predict_proba(data_test)[:,1]

# 평가
get_clf_eval(target_test, preds, pred_proba)

"""#Ensemble"""

from sklearn import ensemble

# trainAcc = []
# testAcc = []

# X_test, Y_test = data_test, target_test

"""#*RandomForest*"""

n_estimator = [10, 20, 50, 100, 200, 300, 500]
testF1 = []
bestf1 = []

for k in n_estimator:
    clf = ensemble.RandomForestClassifier(n_estimators=k, random_state=22)
    clf.fit(data_train, target_train)
    Y_predTrain = clf.predict(data_train)
    Y_predTest = clf.predict(data_test)
    testF1.append(f1_score(target_test, Y_predTest, average='micro'))

plt.figure(figsize=(15, 6))
plt.plot(n_estimator, testF1, 'bv--')
plt.xlabel('RandomForest')
plt.ylabel('F1 score')

bestf1.append(max(testF1))

print("RF 훈련 세트 정확도: {:.3f}".format(clf.score(data_train, target_train)))
print("RF 테스트 세트 정확도: {:.3f}".format(clf.score(data_test, target_test)))

# 예측
preds = clf.predict(data_test)
pred_proba = clf.predict_proba(data_test)[:,1]

# 평가
get_clf_eval(target_test, preds, pred_proba)

"""#Bagging"""

numBaseClassifiers = [100, 200, 300, 500]
max_depths = [2, 3, 5, 10, 20]
F1 = []

plt.figure(figsize=(15, 6))
for k in numBaseClassifiers:
    for maxdepth in max_depths:
        clf = ensemble.BaggingClassifier(DecisionTreeClassifier(max_depth=maxdepth), n_estimators=k, random_state=22)
        clf.fit(data_train, target_train)
        Y_predTrain = clf.predict(data_train)
        Y_predTest = clf.predict(data_test)
        F1.append(f1_score(target_test, Y_predTest, average='micro'))
    plt.plot(max_depths, F1)
    testF1.append(max(F1))
    F1 = []

plt.legend(numBaseClassifiers)
plt.xlabel('max_depth')
plt.ylabel('F1 score')

bestf1.append(max(testF1))

Y_predTest

print("Bagging 훈련 세트 정확도: {:.3f}".format(clf.score(data_train, target_train)))
print("Bagging 테스트 세트 정확도: {:.3f}".format(clf.score(data_test, target_test)))

# 예측
preds = clf.predict(data_test)
pred_proba = clf.predict_proba(data_test)[:,1]

# 평가
get_clf_eval(target_test, preds, pred_proba)

"""#Adaboost"""

numBaseClassifiers = [50, 100, 200, 300, 500]
max_depths = [1, 2, 3, 5, 10, 20]
F1 = []
testF1 = []

plt.figure(figsize=(15, 6))
for k in numBaseClassifiers:
    for maxdepth in max_depths:
        clf = ensemble.AdaBoostClassifier(DecisionTreeClassifier(max_depth=maxdepth), n_estimators=k, random_state=22)
        clf.fit(data_train, target_train)
        Y_predTrain = clf.predict(data_train)
        Y_predTest = clf.predict(data_test)
        F1.append(f1_score(target_test, Y_predTest, average='micro'))

    plt.plot(max_depths, F1)
    testF1.append(max(F1))
    F1 = []

plt.legend(numBaseClassifiers)
plt.xlabel('max_depth')
plt.ylabel('F1 score')

bestf1.append(max(testF1))

print("AdaBoost 훈련 세트 정확도: {:.3f}".format(clf.score(data_train, target_train)))
print("AdaBoost 테스트 세트 정확도: {:.3f}".format(clf.score(data_test, target_test)))

# 예측
preds = clf.predict(data_test)
pred_proba = clf.predict_proba(data_test)[:,1]

# 평가
get_clf_eval(target_test, preds, pred_proba)

"""임계치설정"""

dt_clf = DecisionTreeClassifier(max_depth=testF1.index(max(testF1))+1, random_state=22)
knn_clf =  KNeighborsClassifier(n_neighbors=k, metric='minkowski')
rf_clf = ensemble.RandomForestClassifier(n_estimators=k, random_state=22)
bag_clf =ensemble.BaggingClassifier(DecisionTreeClassifier(max_depth=maxdepth), n_estimators=k, random_state=22)
ada_clf =  ensemble.AdaBoostClassifier(DecisionTreeClassifier(max_depth=maxdepth), n_estimators=k, random_state=22)

model_all = [dt_clf
             , knn_clf, rf_clf, bag_clf, ada_clf]
lr_final = LogisticRegression()

for x in model_all:
    x.fit(data_train, target_train)

thresholds = np.arange(0,1,0.1)

f1 = []

for x in model_all:
    print(x)
    pred_proba=x.predict_proba(np.array(data_test))
    pred_proba_1 = pred_proba[:,1].reshape(-1,1)
    for threshold in thresholds:
        binarizer = Binarizer(threshold=threshold)
        model_cl_pred = binarizer.transform(pred_proba_1)
        print('threshold : ', threshold)
        print('오차 행렬')
        confusion = confusion_matrix(target_test, model_cl_pred)
        print(confusion)
        print('accuracy : ', accuracy_score(target_test,model_cl_pred))
        print('precision : ', precision_score(target_test,model_cl_pred))
        print('recall : ', recall_score(target_test,model_cl_pred))
        print('f1_score : ', f1_score(target_test,model_cl_pred))
        f1.append( f1_score(target_test,model_cl_pred))

        print('')
    print('===================================================')
    print('')

max_threshold = max(f1)
max(f1)

thresholds = np.arange(0,1,0.1)

for x in model_all:
    print(x)
    pred_proba=x.predict_proba(np.array(data_test))
    pred_proba_1 = pred_proba[:,1].reshape(-1,1)
    for threshold in thresholds:
        binarizer = Binarizer(threshold=threshold)
        model_cl_pred = binarizer.transform(pred_proba_1)
        print('threshold : ', threshold)
        print('오차 행렬')
        confusion = confusion_matrix(target_test,model_cl_pred)
        print(confusion)
        print('accuracy : ', accuracy_score(target_test,model_cl_pred))
        print('precision : ', precision_score(target_test,model_cl_pred))
        print('recall : ', recall_score(target_test,model_cl_pred))
        print('f1_score : ', f1_score(target_test,model_cl_pred))
        print('')
    print('===================================================')
    print('')